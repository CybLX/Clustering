#!/usr/bin/env python
# coding: utf-8

# # Sum√°rio
# 
# 1. [Introdu√ß√£o](#Se√ß√£o-1:-Introdu√ß√£o)
# 2. [Feature Engineering](#Se√ß√£o-2:-Feature-Engineering)  
#    2.1 [Statistics](#Se√ß√£o-21:-Statistics)  
#    2.2 [Principal component analysis (PCA)](#Se√ß√£o-22:-Principal-component-analysis-(PCA))
# 3. [Algoritmos](#Se√ß√£o-3:-Algoritmos)  
#    3.1 [K-Means](#Se√ß√£o-31:-K-Means)  
#    3.2 [DBSCAN](#Se√ß√£o-32:-DBSCAN)  
#    3.3 [K - NearestNeighbors](#Se√ß√£o-33:-K---NearestNeighbors)  
#    3.4 [Modelos de Misturas Gaussianas (GMM)](#Se√ß√£o-34:-Modelos-de-Misturas-Gaussianas-(GMM))  
#    3.5 [Agglomerative Clustering](#Se√ß√£o-35:-Agglomerative-Clustering)
# 4. [Conclus√£o](#Se√ß√£o-4:-Conclus√£o)

# # Se√ß√£o 1: Introdu√ß√£o
# 
# A respectiva resenha tem como motiva√ß√£o um conjunto de dados da Copa do Mundo da FIFA de 1930 a 2018 para modelagem. Para caracteriz√°-las, foram informados 86 sele√ß√µes diferentes e diversos dados sobre elas. Iremos avaliar os problemas do aprendizado de m√°quina n√£o supervisionado, trabalhando com conjuntos de dados sem utilizar r√≥tulos ou qualquer tipo de informa√ß√£o sobre as inst√¢ncias manipuladas (Zhi e Goldberg, 2009). As principais tarefas aqui apresentadas nesta se√ß√£o ser√£o: a redu√ß√£o de dimensionalidade e a engenharia de dados.
# 
# 
# ## Se√ß√£o 1.1: An√°lise de agrupamento (Clusteriza√ß√£o)
# 
# A an√°lise de agrupamento √© uma t√©cnica multivariada que, por meio de m√©todos num√©ricos, agrupa vari√°veis em k grupos distintos, onde cada grupo cont√©m n objetos descritos por m caracter√≠sticas. O objetivo √© segmentar os k grupos, com ùëò ‚â™ ùëõ, de forma que os objetos dentro de um mesmo grupo sejam semelhantes entre si e diferentes dos objetos de outros grupos. A medida de dissimilaridade utilizada considera as m caracter√≠sticas dispon√≠veis no conjunto de dados original para determinar se os objetos s√£o similares ou n√£o (Jain e Dubes, 1988; Tan et al., 2006).
# 
# A escolha dessa medida de dissimilaridade √© crucial para a defini√ß√£o adequada dos grupos e envolve a considera√ß√£o de diversos fatores, como atributos quantitativos, qualitativos nominais e ordinais, ou uma combina√ß√£o desses tipos. Para atributos quantitativos, muitas medidas de dissimilaridade s√£o baseadas na dist√¢ncia de Minkowski, que serve como base para diversos algoritmos de agrupamento.
# 
# 
# ## Se√ß√£o 1.2: Dist√¢ncia de Minkowski:
#  Considerando dois objetos $X_i$ e $X_j$, essas distancias sao definidads como:
# $
# \begin{equation}
# d(x_i, x_j) = \left( \sum_{l=1}^{p} |x_{il} - x_{jl}|^p \right)^{\frac{1}{p}}
# \end{equation}
# $
# 
# A escolha do valor de p define varia√ß√µes para essa medida. Dentre elas, os tr√™s casos mais
# conhecidos s√£o [Faceli et al. 2011]:
# 
# - Dist√¢ncia de Manhattan ($P=1$)
# - Dist√¢ncia Euclidiana ($P=2$)
# - Dist√¢ncia de Chebyshev ($P=\inf$)
# 
# Importante salientar que n√£o h√° uma defini√ß√£o universal para a constitui√ß√£o de um grupo (Everitt et al. 2001, Faceli et al. 2011). Dessa forma, cada problema depende do algoritmo e da aplica√ß√£o estudada. Distintamente de aplica√ß√µes de Classifica√ß√£o, a Clusteriza√ß√£o √© uma t√©cnica na qual nenhuma suposi√ß√£o √© feita a respeito dos grupos, por√©m, formalmente devem satisfazer as propriedades de uma parti√ß√£o r√≠gida. Sendo elas:
# 
# Formalmente, dado um conjunto $X = {x_1 , ¬∑ ¬∑ ¬∑ , x_n}$, uma parti√ß√£o r√≠gida consiste em
# uma cole√ß√£o de subconjuntos $C = {C_1, ¬∑ ¬∑ ¬∑ ,C_k}$, satisfazendo as seguintes propriedades
# [Xu e Wunsch 2005]:
# - $C_1 \cup C_2 \cup \cdots \cup C_k = X$;
# - $C_i \neq \emptyset, \, \forall i \in [1, k]$; e
# - $C_i \cap C_j = \emptyset, \, \forall i, j \in [1, k] \text{ e } i \neq j$.
# 

# In[1]:


from sklearn.metrics import silhouette_score,davies_bouldin_score, calinski_harabasz_score
from sklearn.preprocessing import scale, minmax_scale, MinMaxScaler, LabelEncoder
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import scipy.cluster.hierarchy as shc
from itertools import combinations
import matplotlib.pyplot as plt
from kneed import KneeLocator
import seaborn as sns
import pandas as pd
import numpy as np
import glob
#https://kneed.readthedocs.io/en/stable/api.html#kneelocator


# In[ ]:


# Download and unzip files
get_ipython().system('wget https://github.com/cmcouto-silva/datasets/raw/main/datasets/fifa-football-world-cup-dataset.zip')
get_ipython().system('unzip fifa-football-world-cup-dataset.zip')


# In[2]:


# Listando arquivos
files = glob.glob('fifa-football-world-cup-dataset/FIFA - *')


# In[3]:


def read_csv(file):
  df = pd.read_csv(file)
  df['Year'] = int(file.split(' ')[-1].split('.')[0])
  return df


_ = [read_csv(file) for file in files]
df_teams = pd.concat(_)
df_teams.head()


# # Se√ß√£o 2: Feature Engineering
# ## Se√ß√£o 2.1: Statistics

# In[4]:


# feature engineering

df_teams = (
  df_teams
 .assign(**{
     'Win %': lambda x: x['Win'] / x['Games Played'],
     'Draw %': lambda x: x['Draw'] / x['Games Played'],
     'Loss %': lambda x: x['Loss'] / x['Games Played'],
     'Avg Goals For': lambda x: x['Goals For'] / x['Games Played'],
     'Avg Goals Against': lambda x: x['Goals Against'] / x['Games Played'],
 })
)

df_teams['Rank'] = df_teams.groupby('Year')['Position'].transform(lambda x: 1 - minmax_scale(x))
df_teams['Goal Difference'] = df_teams['Goal Difference'].apply(lambda x: str(x).replace('‚àí', '-').strip()).astype(int)
df_teams.reset_index(inplace = True, drop=True)

# Saving teams code
teams_code = LabelEncoder()
teams_code.fit(df_teams['Team'])
df_teams['Team'] = teams_code.transform(df_teams['Team'])
df_teams = df_teams.astype(float)


# In[5]:


df_teams.info()


# In[6]:


# Agrupando pelos times

df_teams_stats = df_teams.groupby(['Team']).agg(
  n_cups = ('Team', 'count'),
  avg_wins = ('Win %', 'mean'),
  avg_draws = ('Draw %', 'mean'),
  avg_losses = ('Loss %', 'mean'),
  avg_goals_for = ('Avg Goals For', 'mean'),
  avg_goals_against = ('Avg Goals Against', 'mean'),
  avg_rank = ('Rank', 'mean')
)


# In[7]:


df_teams_stats.head()


# In[8]:


# Verificando outliers
df_teams_stats.apply(scale).boxplot()
plt.xticks(rotation=60, ha='right')
plt.show()

# Filtrando outliers por 3 std
for col in df_teams_stats.columns:
  avg,std = df_teams_stats[col].agg(['mean','std'])
  df_teams_stats[col] = df_teams_stats[col].clip(lower=avg-3*std, upper=avg+3*std)


# In[9]:


df_teams_stats.head()


# In[10]:


# Criando um Check Point
teams = df_teams_stats.copy()


# Assim, vamos interpretar nossos dados utilizando o Pandas e aplicar uma normaliza√ß√£o dos dados para que seu menor valor fique -1 e maior +1.

# In[11]:


# Normalizando valores para para uma mesma escala (menor valor ser√° -1 e maior 1)
teams = teams.astype(float)
scaler = MinMaxScaler(feature_range=(-1,1))
teams[:] = scaler.fit_transform(teams)



# "Outra t√©cnica [‚Ä¶] √© calcular a m√©dia estat√≠stica e o desvio padr√£o dos valores dos atributos, subtrair a m√©dia de cada valor e dividir o resultado pelo desvio padr√£o. Esse processo √© chamado de padroniza√ß√£o de uma vari√°vel estat√≠stica e resulta em um conjunto de valores cuja m√©dia √© zero e o desvio padr√£o √© um."
# ‚Äî P√°gina 61, Minera√ß√£o de Dados: Ferramentas e T√©cnicas Pr√°ticas de Aprendizado de M√°quina , 2016.
# 

# In[12]:


#Vamos montar tambem algumas funcoes para visualizacao dos dados e da matriz de correlacao:

def figures(sensores,title, color = ['r', 'g', 'm']):
    coluns = sensores.columns.drop('labels')
    comb_sens = combinations(coluns, 2)
    for comb in comb_sens:
        fig = plt.figure(figsize=(5, 5))
        ax = fig.add_subplot(111)
        x = sensores[f'{comb[0]}']
        y = sensores[f'{comb[1]}']
        plt.title(f'{title}')
        sns.scatterplot(data=sensores, x=comb[0],y=comb[1],hue='labels')
        ax.set_xlabel(f'Cluster {comb[0]}')
        ax.set_ylabel(f'Cluster {comb[1]}')
    return plt.show()

def matrix_corr(db, title):
    matriz_Ms = pd.DataFrame(db).corr()

    plt.figure(figsize=(10, 16))
    plt.title(f'{title}', fontsize=16)

    sns.heatmap(matriz_Ms,
                annot=True,           
                cmap='coolwarm',       
                xticklabels=matriz_Ms.columns,
                yticklabels=matriz_Ms.columns)

    plt.tight_layout()  
    plt.show()
    plt.close()

def paises_por_cluster(df):
    table_data = {}
    for label in df['labels'].unique():
        index_list = df[df['labels'] == label]['index'].tolist()
        table_data[label] = index_list
    
    table_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in table_data.items()]))
    
    display(table_df)

def boxplot_paises(df):
    required_columns = ['labels', 'n_cups', 'avg_wins', 'avg_draws', 
                        'avg_losses', 'avg_goals_for', 
                        'avg_goals_against', 'avg_rank', 'index']
    
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"Colunas not found {required_columns}")

    sns.set(style="darkgrid", palette="deep")
    plt.style.use('dark_background')

    variables = ['n_cups', 'avg_wins', 'avg_draws', 
                 'avg_losses', 'avg_goals_for', 
                 'avg_goals_against', 'avg_rank']
    
    plt.figure(figsize=(20, 12))
    
    for i, var in enumerate(variables, 1):
        plt.subplot(3, 3, i)
        sns.boxplot(x='labels', y=var, data=df, hue='labels', palette='Set2', legend=False)
        plt.title(f'Boxplot de {var} por Label', fontsize=16, color='white')
        plt.xlabel('Label', fontsize=14, color='white')
        plt.ylabel(var, fontsize=14, color='white')
        plt.xticks(fontsize=12, color='white')
        plt.yticks(fontsize=12, color='white')
        plt.grid(color='grey')
    plt.tight_layout(pad=3.0)
    plt.show()



# A matriz de correla√ß√£o abaixo quantifica em uma rela√ß√£o linear das colunas entre elas mesmas. O c√°lculo da correla√ß√£o √© feito usando o coeficiente de Pearson, que varia de -1 a 1. Um valor de 1 indica uma correla√ß√£o positiva perfeita, -1 uma correla√ß√£o negativa perfeita, e 0 nenhuma correla√ß√£o. O resultado gera um matriz de correla√ß√£o √∫til para compararmos as caracter√≠sticas entre si.

# In[13]:


matrix_corr(teams, 'Fifa World Cup Correlation')


# ## Se√ß√£o 2.2: Principal component analysis (PCA)
# ### Redu√ß√£o da dimensionalidade
# 
# A An√°lise de Componentes Principais (PCA) √© uma t√©cnica eficaz para identificar padr√µes e diferen√ßas em um conjunto de dados. Ao identificar essas caracter√≠sticas, conseguimos comprimir os dados em um espa√ßo menor, sem perder uma parte significativa de sua vari√¢ncia. Essencialmente, o PCA projeta os dados em um subespa√ßo formado por autovetores (eixos ortogonais), o que resulta em uma redu√ß√£o linear da dimensionalidade, usando a decomposi√ß√£o de valor singular (SVD). Para isso, utilizamos a implementa√ß√£o LAPACK do SVD completo ou o SVD truncado randomizado, conforme o m√©todo de Halko et al. (2009).
# 
# Podemos aplicar o PCA ao conjunto de dados para reduzir o n√∫mero de vari√°veis, agrupando-as em componentes mais amplas e significativas. Esses componentes ser√£o ortogonais entre si, preservando at√© 95% da vari√¢ncia original. Essa t√©cnica √© especialmente √∫til para grandes conjuntos de dados cont√≠nuos e multivariados, onde n√£o h√° uma rela√ß√£o linear evidente entre as vari√°veis.
# 
# No c√≥digo a seguir, realizamos testes para reduzir os componentes a at√© 7 dimens√µes, analisando a soma acumulada de vari√¢ncia para cada componente. Em seguida, determinamos o n√∫mero ideal de componentes que melhor preserva 95% da vari√¢ncia e aplicamos a redu√ß√£o para esse n√∫mero.

# In[14]:


evr = []
for i in range(1,7):
    principal=PCA(n_components=i)
    data = principal.fit(teams)
    evr.append(principal.explained_variance_ratio_.sum()) # Soma da variacia total acumulada para o respectivo n_components

plt.plot(range(1, 7), evr, marker = 'o', linestyle = '--')
plt.title('The Explained Variance Ratio')
plt.xlabel('Number of Dimmensions')
plt.ylabel('Variance Ratio')
plt.show()


# Segue o novo conjunto de vetores para os dados originais:
# Preservando 95% da vari√¢ncia dos dados

# In[15]:


arr = np.abs(np.array(evr)- 0.95) #  Preservar 95% da variancia dos dados
indice = np.where(arr == np.amin(arr)) # Identifica a quantidade n_components
principal=PCA(n_components = indice[0][0])# Reduz a dimenssao dos sensores
vec_ = pd.DataFrame(principal.fit_transform(teams), columns = ['P{}'.format(col) for col in range(0,indice[0][0])]) # Novos vetores
vec_.head()


# Com a redu√ß√£o dos sensores, √© poss√≠vel determinar a qual cluster cada ponto pertence, classificando-os e relacionando-os a cada vari√°vel independente. As pontua√ß√µes rec√©m-obtidas do PCA ser√£o incorporadas aos algoritmos K-Means, K-Nearest Neighbors, DBScan, Agglomerative Clustering e Gaussian mixture models para realizar as segmenta√ß√µes e classifica√ß√µes.

# # Se√ß√£o 3: Algoritmos
# ## Se√ß√£o 3.1: K-Means
# 
# O algoritmo K-means agrupa os dados tentando separ√°-los em n grupos de vari√¢ncia semelhante, minimizando o crit√©rio de in√©rcia ou soma dos quadrados (WCSS) para cada solu√ß√£o. Ele divide o conjunto de N amostras em K clusters disjuntos, sendo cada um representado pela m√©dia das amostras dentro do cluster, conhecidos como centr√≥ides. Esse m√©todo √© conhecido como Elbow, onde decidimos quantos clusters manter com base na forma da curva.
# 
# A in√©rcia n√£o √© uma m√©trica normalizada; sabemos apenas que valores menores, pr√≥ximos de zero, s√£o ideais. Em espa√ßos de alta dimens√£o, as dist√¢ncias euclidianas podem se tornar infladas (a chamada maldi√ß√£o da dimensionalidade). Por isso, aplicamos a redu√ß√£o de dimensionalidade utilizando as pontua√ß√µes do PCA.
# 
# #### Como ele funciona:
# 
# O K-means √© frequentemente chamado de algoritmo de Lloyd e opera em tr√™s etapas principais.
# 
# Na primeira etapa, s√£o escolhidos os centr√≥ides iniciais, selecionando amostras do pr√≥prio conjunto de dados. Ap√≥s essa inicializa√ß√£o, o algoritmo alterna entre as duas etapas seguintes: a atribui√ß√£o de cada amostra ao centr√≥ide mais pr√≥ximo.
# 
# A segunda etapa, novos centr√≥ides s√£o calculados tomando a m√©dia de todas as amostras associadas ao centr√≥ide anterior. A diferen√ßa entre o centr√≥ide antigo e o novo √© ent√£o calculada, e o processo se repete at√© que essa diferen√ßa seja menor que um valor pr√©-definido, ou seja, at√© que os centr√≥ides n√£o mudem significativamente.
# 
# Ap√≥s coletar os valores de in√©rcia para o primeiro cluster at√© o s√©timo, calculamos os coeficientes de Silhueta e outras m√©tricas.
# 
# ### Se√ß√£o 3.1.1: M√©tricas
# 
# #### silhouette score
# O Coeficiente de Silhueta √© calculado com base na dist√¢ncia m√©dia dentro do pr√≥prio cluster e na dist√¢ncia m√©dia at√© o cluster mais pr√≥ximo, do qual a amostra n√£o faz parte. Esse coeficiente mede o qu√£o bem uma amostra est√° associada ao seu pr√≥prio cluster em compara√ß√£o com outros clusters. A f√≥rmula utilizada √©:
# $
# \begin{equation}
# \frac{(b - a)}{\max(a, b)}
# \end{equation}
# $
# 
# Onde "a" √© a dist√¢ncia m√©dia intra-cluster e "b" √© a dist√¢ncia at√© o cluster mais pr√≥ximo. O coeficiente de silhueta s√≥ √© definido para:
# 
# 
# $
# \begin{equation}
# 2 \leq n_{\text{labels}} \leq n_{\text{samples}} - 1
# \end{equation}
# $
# 
# 
# O melhor valor √© 1, enquanto o pior √© -1. Valores pr√≥ximos de 0 indicam que os clusters est√£o sobrepostos, e valores negativos sugerem que a amostra foi atribu√≠da ao cluster errado, pois pertence a um cluster mais semelhante.
# 
# https://scikit-learn.org/dev/modules/clustering.html#silhouette-coefficient
# 
# #### Davies Bouldin Score
# 
# O √≠ndice de Davies-Bouldin (DB) √© uma m√©trica que avalia a qualidade dos clusters gerados, sendo baseado na compara√ß√£o entre a dist√¢ncia intra-cluster e a dist√¢ncia entre os centr√≥ides dos clusters. Quanto menor o valor do √≠ndice, melhor a separa√ß√£o entre os clusters. 
# 
# Uma das vantagens do √≠ndice DB √© sua simplicidade de c√°lculo em compara√ß√£o com o coeficiente de silhueta, pois utiliza apenas as dist√¢ncias entre os pontos do dataset. No entanto, ele tende a ter valores mais altos para clusters convexos, sendo menos eficaz para clusters de densidade, como os gerados pelo DBSCAN.
# 
# A formula√ß√£o matem√°tica do √≠ndice DB √© dada por:
# 
# $
# \begin{equation}
# DB = \frac{1}{k} \sum_{i=1}^{k} \max_{i \neq j} \left( \frac{S_i + S_j}{d_{ij}} \right)
# \end{equation}
# $
# 
# Onde:
# - $(S_{i})$ √© a dist√¢ncia m√©dia entre os pontos do cluster $( C_{i} )$ e seu centr√≥ide (di√¢metro do cluster).
# - $(d_{ij})$ √© a dist√¢ncia entre os centr√≥ides dos clusters $(C_i)$ e $(C_j)$.
# 
# https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index
# 
# #### Calinski Harabasz Score
# 
# O √≠ndice de Calinski-Harabasz, tamb√©m conhecido como Crit√©rio de Raz√£o de Vari√¢ncia, avalia a qualidade dos clusters considerando a dispers√£o entre e dentro dos clusters. Um valor maior indica clusters bem definidos. 
# 
# A f√≥rmula para o √≠ndice √© dada como a raz√£o entre a dispers√£o entre clusters e a dispers√£o dentro dos clusters:
# 
# $
# \begin{equation}
# s = \frac{tr(B_k)}{tr(W_k)} \times \frac{(n_E - k)}{(k - 1)}
# \end{equation}
# $
# 
# Onde:
# 
# $
# \begin{equation}
# W_k = \sum_{q=1}^{k} \sum_{x \in C_q} (x - c_q)(x - c_q)^T
# \end{equation}
# $
# 
# $
# \begin{equation}
# B_k = \sum_{q=1}^{k} n_q (c_q - c_E)(c_q - c_E)^T
# \end{equation}
# $
# 
# Aqui, $(c_{q})$ √© o centro do cluster $(q)$, $(c_{E})$ √© o centro do conjunto de dados $(E)$, e $(n_{q})$ √© o n√∫mero de pontos no cluster $(q)$.
# 
# https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index

# In[16]:


# Aplicando o algoritmo KMeans para diferentes n√∫meros de clusters
metrics = silhouette_score, davies_bouldin_score, calinski_harabasz_score
metrics_results = []
wcss = []
for i in range(3, 10):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 200, n_init = 25, tol = 0.001)#, random_state = 50)
    kmeans.fit(vec_)

    results = {'k':i}
    results['wcss'] = kmeans.inertia_
    wcss.append(kmeans.inertia_)
    cluster_labels = kmeans.labels_
    for metric in metrics:
        results[metric.__name__] = metric(vec_, cluster_labels)
    
    metrics_results.append(results)


plt.plot(range(3, 10), wcss, marker = 'o', linestyle = '--')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

pd.DataFrame(metrics_results).set_index('k').style.background_gradient(cmap='Blues')


# In[17]:


(
  pd.DataFrame(metrics_results)
  .set_index('k').style.background_gradient(cmap='Oranges', subset='wcss')
  .highlight_max(subset=['silhouette_score','calinski_harabasz_score'])
  .highlight_min(subset='davies_bouldin_score')
)


# Notamos que o n√∫mero ideal de clusters, com os melhores scores, varia entre 3 e 4 ao utilizar o algoritmo KMeans. Assim, podemos classificar tanto os vetores PCA quanto os times originais em 3 ou 4 clusters.

# In[18]:


kmeans = KMeans(n_clusters=3, init = 'k-means++', max_iter=500)
kmeans.fit_predict(teams)

# Classificando os vetores PCA
vec_['labels'] = kmeans.labels_
vec_.head()


# In[19]:


# Preparando os dados originais para a classifica√ß√£o
df_teams_stats.index = teams_code.inverse_transform(df_teams_stats.index.astype(int))
df_teams_stats.reset_index(inplace = True, drop = False)


# In[20]:


# Classica os paises em grupos pelo KMeans
df_teams_stats['labels'] = kmeans.labels_
grouped = df_teams_stats.groupby('labels')['index'].apply(list).reset_index()
grouped['count'] = grouped['index'].apply(len)


plt.figure(figsize=(10, 6))
plt.bar(grouped['labels'], grouped['count'], color='black')
plt.title('N√∫mero de Pa√≠ses por Cluster')
plt.xlabel('Labels')
plt.ylabel('N√∫mero de Pa√≠ses')
plt.xticks(grouped['labels'])
plt.grid(axis='y')
plt.show()


# Observe a correlacao dos vetores PCA com as suas classificacoes

# In[21]:


matrix_corr(vec_, title = 'Cluester KMeans')


# Tamb√©m podemos observar a correla√ß√£o dos dados originais com os clusters

# In[22]:


#_ =  pd.concat([teams, vec_], axis = 1)

teams['labels'] = kmeans.labels_
matrix_corr(teams, title = 'Sensores originais')


# Podemos tamb√©m visualizar a lista de pa√≠ses por cluster e comparar as caracter√≠sticas de cada um deles.

# In[23]:


boxplot_paises(df_teams_stats)
paises_por_cluster(df_teams_stats)


# #### COMPLEMENTO C√ÅLCULO EUCLIDIANO DA DISTANCIA ENTRE OS PONTOS.
# 

# In[24]:


# aproxima√ß√£o direta
import math

def calculate_kn_distance(X,k):

    kn_distance = []
    for i in range(len(X)):
        eucl_dist = []
        for j in range(len(X)):
            eucl_dist.append(
                math.sqrt(
                    ((X[i,0] - X[j,0]) ** 2) +
                    ((X[i,1] - X[j,1]) ** 2)))

        eucl_dist.sort()
        kn_distance.append(eucl_dist[k])

    plt.figure(figsize=(8, 5))  
    plt.hist(kn_distance, bins=30)
    plt.ylabel('n')
    plt.xlabel('Epsilon distance')
    plt.title('Histogram of Epsilon Distance')
    plt.tight_layout()
    plt.show()  
    return kn_distance

eps_dist = calculate_kn_distance(vec_.values, 3)


# Podemos tamb√©m identificar o ponto de joelho, que √© definido como o ponto de m√°xima curvatura em um sistema. Reconhecer essa localiza√ß√£o pode ser √∫til em diversas situa√ß√µes, mas, no contexto de aprendizado de m√°quina, √© particularmente valioso para auxiliar na escolha de um valor apropriado de $k$ na t√©cnica de agrupamento K-means.
# 
# Essa abordagem permite otimizar o n√∫mero de clusters, aumentando a efici√™ncia e a interpretabilidade dos resultados da an√°lise dos dados. Para isso, utilizaremos um reposit√≥rio espec√≠fico para localizar esse ponto.
# 
# Uma vez instanciada, a classe `KneeLocator` tenta encontrar o ponto de curvatura m√°xima em uma linha. O joelho √© acess√≠vel atrav√©s do atributo .knee .

# In[25]:


kneedle = KneeLocator(range(1,len(eps_dist) +1),  #x values
                        eps_dist, # y values
                        S=1.0, #parameter suggested from paper
                        curve="convex", #parameter from figure
                        direction="decreasing") #parameter from figure
print(f"Knee at y = {kneedle.knee_y}")


plt.figure(figsize=(8, 5)) 
kneedle.plot_knee_normalized()
plt.title('Kneedle - Normalized Plot')
plt.tight_layout() 
plt.show()


# ## Se√ß√£o 3.2: DBSCAN 
# ### Algoritmo de Agrupamento Baseado em Densidade
# 
# O DBScan √© um algoritmo de agrupamento que se baseia na densidade dos dados. Para identificar regi√µes densas no espa√ßo, o DBScan utiliza dois par√¢metros principais: MinPts (n√∫mero m√≠nimo de vizinhos) e Eps (dist√¢ncia m√≠nima entre pontos de clusters distintos). Esses par√¢metros s√£o fundamentais para reconhecer os pontos de maior densidade dentro do conjunto de dados.
# 
# Diferentemente de outros algoritmos, o DBScan n√£o fornece um valor √≥timo para a dist√¢ncia entre amostras. Para isso, utilizamos o algoritmo NearestNeighbors para determinar esses par√¢metros. O DBScan, que significa "Agrupamento Espacial de Aplica√ß√µes com Ru√≠do Baseado em Densidade", expande clusters a partir de pontos que atendem aos crit√©rios de densidade estabelecidos.
# 
# A ideia central do DBScan √© que, para cada cluster, h√° uma quantidade m√≠nima de pontos dentro de um determinado raio. A densidade de um ponto vizinho p em rela√ß√£o a q √© medida pela k-dist√¢ncia, que define o limite que separa o ru√≠do (√† esquerda do limiar k) e o cluster (√† direita do limiar k). Portanto, o DBScan utiliza valores globais para Eps e MinPts, aplicando os mesmos valores a todos os clusters. Esses par√¢metros de densidade do cluster mais "fino" s√£o bons candidatos para especificar os limites abaixo dos quais a densidade n√£o √© considerada ru√≠do.
# 
# Para identificar o ru√≠do em nossos dados, podemos calcular diretamente os dois par√¢metros do algoritmo: MinPts e Eps. O objetivo √© encontrar o maior MinPts e o menor Eps. Para isso, calculamos a dist√¢ncia m√©dia intra-cluster e a dist√¢ncia m√©dia do cluster mais pr√≥ximo para cada amostra, utilizando o Coeficiente de Silhueta. Este coeficiente mede a dist√¢ncia entre uma amostra e o cluster mais pr√≥ximo do qual essa amostra n√£o faz parte.

# In[26]:


# busca por Forca Bruta: Alto nivel de processamento, muito custoso.
min_samples = range(3,50)
eps = np.arange(0.2,5., 0.01)

output = []
for ms in min_samples:
    for ep in eps:
        labels = DBSCAN(min_samples=ms, eps = ep).fit(vec_).labels_
        try:
            score = silhouette_score(vec_, labels)
            output.append((ms, ep, score))
        except:
            pass


min_samples, eps, score = sorted(output, key=lambda x:x[-1])[-1] # ordena do menor para o maior score, selecionando-o
print(f"Best silhouette_score: {score}")
print(f"min_samples: {min_samples}")
print(f"eps: {eps}")
print("*"*20)



# Com isso, estabelecemos a quantidade m√≠nima de pontos necess√°ria para a menor dist√¢ncia entre uma amostra e o cluster do qual ela n√£o pertence. Assim, podemos rotular nossos pontos de acordo com a presen√ßa de ru√≠do e o cluster ao qual est√£o associados.

# In[27]:


labels = DBSCAN(min_samples=18, eps = 1.0600000000000007).fit(vec_).labels_

print(f"Number of clusters totais: {len(set(labels))}")
print(f"Number of clusters, ignoring noise: {len(set(labels)) - (1 if -1 in labels else 0)}")
print(f"Number of outliers/noise: {list(labels).count(-1)}")
print(f"Silhouette_score: {silhouette_score(teams, labels)}")

vec_['labels'] = labels
figures(vec_, title = 'DBScan Clusters')


# Note que algumas compara√ß√µes apresentam clusters bem definidos, enquanto outras parecem estar sobrepostos. Podemos ent√£o classificar os dados originais.

# In[28]:


# Classica os paises em grupos pelo KMeans
df_teams_stats['labels'] = labels
grouped = df_teams_stats.groupby('labels')['index'].apply(list).reset_index()
grouped['count'] = grouped['index'].apply(len)

# Plotar o gr√°fico
plt.figure(figsize=(10, 6))
plt.bar(grouped['labels'], grouped['count'], color='skyblue')
plt.title('N√∫mero de Pa√≠ses por Cluster')
plt.xlabel('Labels')
plt.ylabel('N√∫mero de Pa√≠ses')
plt.xticks(grouped['labels'])
plt.grid(axis='y')

plt.show()


# In[29]:


boxplot_paises(df_teams_stats)
paises_por_cluster(df_teams_stats)


# Podemos tamb√©m adotar uma abordagem mais heur√≠stica utilizando o algoritmo K-Nearest Neighbors para determinar a melhor dist√¢ncia entre vizinhos (eps) com base no ponto de joelho. Esse m√©todo reduz o tempo e o custo de processamento necess√°rios para definir os par√¢metros do DBSCAN.

# ## Se√ß√£o 3.3: K - NearestNeighbors
# 
# 
# https://scikit-learn.org/stable/modules/neighbors.html
# 
# O algoritmo K-Nearest Neighbors (KNN) √© uma t√©cnica fundamental em aprendizado de m√°quina, sua ideia central √© prever o r√≥tulo de um ponto de dados novo com base na proximidade a um n√∫mero predefinido de amostras de treinamento. Para isso, o algoritmo encontra os k vizinhos mais pr√≥ximos e, a partir desses vizinhos, determina a classe a ser atribu√≠da. A dist√¢ncia entre os pontos pode ser calculada utilizando diferentes m√©tricas, sendo a dist√¢ncia Euclidiana uma das mais comuns.
# 
# Matematicamente, a previs√£o para um novo ponto ùë• √© feita pela fun√ß√£o:
# 
# $
# \begin{equation}
# s = \frac{tr(B_k)}{tr(W_k)} \times \frac{(n_E - k)}{(k - 1)}
# \end{equation}
# $
# 
# onde $ùë¶_ùëñ$ s√£o os r√≥tulos dos k vizinhos mais pr√≥ximos de ùë•. Essa abordagem √© considerada um m√©todo n√£o param√©trico, pois n√£o assume uma forma funcional espec√≠fica para a distribui√ß√£o dos dados, simplesmente "lembra" de todas as amostras de treinamento.
# 
# ### Implementa√ß√£o e Escolha de Par√¢metros
# 
# A implementa√ß√£o do KNN em scikit-learn utiliza tr√™s algoritmos principais: BallTree, KDTree e um algoritmo de for√ßa bruta. Quando o valor padr√£o `auto` √© utilizado, o algoritmo tenta identificar a melhor abordagem com base nos dados de treinamento. A √°rvore de bola (Ball Tree) √© prefer√≠vel para problemas em alta dimensionalidade, pois particiona os dados em hiperesferas aninhadas, permitindo uma busca mais eficiente. A efici√™ncia na busca por vizinhos √© aprimorada pela utiliza√ß√£o da desigualdade do tri√¢ngulo, que reduz o n√∫mero de candidatos para a busca.
# 
# A escolha do valor √≥timo para k deve ser feita com cautela, pois valores altos podem suavizar os limites de decis√£o, enquanto valores baixos podem ser sens√≠veis ao ru√≠do. Por outro lado, a dist√¢ncia $\epsilon$ (em algoritmos como DBSCAN) √© encontrada no ponto de curvatura m√°xima da rela√ß√£o entre o n√∫mero de vizinhos e a dist√¢ncia, podendo ser visualizada graficamente. 
# 
# Em dados n√£o uniformemente amostrados, o classificador `RadiusNeighborsClassifier` pode ser mais eficaz, pois permite que a quantidade de vizinhos utilizados na classifica√ß√£o varie com a densidade local dos dados. A utiliza√ß√£o de pesos para os vizinhos, definida pelo par√¢metro `weights`, tamb√©m melhora a precis√£o das classifica√ß√µes, onde vizinhos mais pr√≥ximos t√™m maior influ√™ncia na decis√£o final.
# 
# Segue abaixo um exemplo com o nosso conjunto de dados:

# In[30]:


output = []
nn = range(3, 25)

fig = plt.figure(figsize=(8, 5))
ax = fig.add_subplot(111)

# Loop para diferentes valores de k vizinhos
for k in nn:
    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean', algorithm = 'brute', n_jobs = -1).fit(vec_)
    distances, indices = nbrs.kneighbors(vec_)
    distances = np.sort(distances, axis=0)  # Ordena as dist√¢ncias de cada ponto
    distances = sorted(distances[:, k-1], reverse=True)  # Pega a k-√©sima dist√¢ncia mais pr√≥xima e ordena
    ax.plot(list(range(1, len(distances) + 1)), distances)
    output.append(distances)

plt.title('Dist√¢ncia dos Pontos')
ax.set_xlabel('Quantidade de Pontos')
ax.set_ylabel('Dist√¢ncia Euclidiana')
ax.grid()
plt.show()
#Lets plot the distances of each point in ascending order of the distance, elbow point will give us the samller range for optimal eps value.


# In[31]:


#https://kneed.readthedocs.io/en/stable/api.html#kneelocator
neighbors = []
for d in output:
    kneedle = KneeLocator(range(1,len(d) +1),  #x values
                          d, # y values
                          S=1.0, #parameter suggested from paper
                          curve="convex", #parameter from figure
                          direction="decreasing") #parameter from figure
    neighbors.append(kneedle.knee_y)

mean = sum(neighbors)/len(neighbors)
arr = np.abs(np.array(neighbors)- mean)
indice = np.where(arr == np.amin(arr))

print(f'The average maximum distance between two neighboring samples is: {mean}')
print(f'The number of samples in a neighborhood: {indice[0]}')


# In[32]:


kneedle = KneeLocator(range(1,len(output[9]) +1),  #x values
                    output[9], # y values
                    S=1.0, #parameter suggested from paper
                    curve="convex", #parameter from figure
                    direction="decreasing") #parameter from figure
plt.figure(figsize=(8, 5)) 
kneedle.plot_knee_normalized()
plt.title('Kneedle - Normalized Plot')
plt.tight_layout() 
plt.show()


# Com os par√¢metros `min_samples` e `eps` definidos pelo algoritmo NearestNeighbors, podemos aplicar o DBSCAN para classificar os clusters, identificando regi√µes densas e expandindo os grupos a partir dos pontos centrais, ou "n√∫cleos".
# 
# Essa abordagem n√£o apenas permite a identifica√ß√£o dos clusters existentes, mas tamb√©m distingue os pontos classificados como ru√≠do.

# In[33]:


labels = DBSCAN(min_samples=9, eps = 1.1722356349857794).fit(teams).labels_

print(f"Number of clusters totais: {len(set(labels))}")
print(f"Number of clusters, ignoring noise: {len(set(labels)) - (1 if -1 in labels else 0)}")
print(f"Number of outliers/noise: {list(labels).count(-1)}")


# Podemos mais uma vez efetuar a classifica√ß√£o nos dados originais.
# 

# In[34]:


# Classica os paises em grupos pelo NearestNeighbors e DBScan
df_teams_stats['labels'] = labels
grouped = df_teams_stats.groupby('labels')['index'].apply(list).reset_index()
grouped['count'] = grouped['index'].apply(len)

# Plotar o gr√°fico
plt.figure(figsize=(10, 6))
plt.bar(grouped['labels'], grouped['count'], color='skyblue')
plt.title('N√∫mero de Pa√≠ses por Cluster')
plt.xlabel('Labels')
plt.ylabel('N√∫mero de Pa√≠ses')
plt.xticks(grouped['labels'])
plt.grid(axis='y')

plt.show()


# In[35]:


vec_['labels'] = labels
figures(vec_, title = 'DBScan Clusters')


# In[36]:


boxplot_paises(df_teams_stats)
paises_por_cluster(df_teams_stats)


# ## Se√ß√£o 3.4: Modelos de Misturas Gaussianas (GMM)
# 
# Um Modelo de Misturas Gaussianas (GMM) √© um modelo estoc√°stico que representa dados como uma combina√ß√£o de m√∫ltiplas distribui√ß√µes gaussianas. No contexto do reconhecimento de locutor, cada classe pode ser interpretada como uma unidade ac√∫stica ou um locutor, onde a informa√ß√£o temporal n√£o √© considerada. O objetivo principal √© calcular a probabilidade a posteriori de uma observa√ß√£o $o$ dada uma classe $C_i$, utilizando a f√≥rmula de Bayes:
# 
# $
# \begin{equation}
# P(C_i | o) = \frac{P(o | C_i) \cdot P(C_i)}{P(o)}
# \end{equation}
# $
# 
# onde $P(o | C_{i})$ √© a probabilidade condicional de observar $o$ dado que ele pertence √† classe $C_{i}$ e $P(C_{i})$ √© a probabilidade a priori da classe $C_{i}$.
# 
# Para um conjunto de $T$ vetores ac√∫sticos observados, onde cada vetor √© considerado um evento independente, podemos maximizar a probabilidade a posteriori ao utilizar a seguinte regra de decis√£o:
# 
# $
# \begin{equation}
# \alpha_{k} = \arg\max_{1 \leq i \leq M} \prod_{t = 1}^T  P(o_t | C_i) P(C_i)
# \end{equation}
# $
# 
# Neste caso, o termo $P(o)$ do denominador da equa√ß√£o anterior pode ser omitido, pois ele √© constante para todas as classes testadas. A modelagem das fun√ß√µes densidade de probabilidade (fdps) dos vetores ac√∫sticos √© crucial para o reconhecimento de locutor, sendo realizada atrav√©s de algoritmos de re-estima√ß√£o de par√¢metros, como o algoritmo de Baum-Welch, tamb√©m conhecido como algoritmo Forward-Backward.
# 
# Os modelos GMM s√£o formalizados pela equa√ß√£o:
# 
# $
# \begin{equation}
# p(o_t | C_j) = p(o_t | \lambda_j) = \sum_{i=1}^{I} c_i \cdot N(o_t; \mu_i, \Sigma_i)
# \end{equation}
# $
# 
# onde $I$ √© o n√∫mero de componentes gaussianas na mistura, $c_i$ representa o peso de cada gaussiana, e $N(o_t; \mu_i, \Sigma_i)$ denota a fun√ß√£o de densidade de probabilidade da gaussiana multivariada com m√©dia $\mu_i$ e matriz de covari√¢ncia $\Sigma_i$. 
# 
# Assim, a probabilidade condicional $p(o_t | C_j)$ √© calculada, sendo utilizada no processo de decis√£o anteriormente mencionado, onde a probabilidade a priori reflete a frequ√™ncia de ocorr√™ncia de cada classe $C_i$ na base de dados.

# In[37]:


# GMM aplicada ao nosso caso
results = []
k_range = range(3,50)
covariance_types = ['full', 'tied', 'diag', 'spherical']

for n_components in k_range:
  for covariance_type in covariance_types:
    mclust = GaussianMixture(n_components=n_components, warm_start=True, covariance_type=covariance_type)
    mclust.fit(vec_)
    results.append({
      'bic': mclust.bic(vec_),
      'n_components': n_components,
      'covariance_type': covariance_type,
    })

results = pd.DataFrame(results)

sns.lineplot(data=results, x='n_components', y='bic', hue='covariance_type', marker='o', alpha=.8)
plt.title('BIC Scores by Number of Components and Covariance Type')
plt.xlabel('Number of Components')
plt.ylabel('BIC Score')
plt.legend(title='Covariance Type')
plt.grid()
plt.show()


# Com o n√∫mero de componentes $n_{components}$ definido e o Crit√©rio de Informa√ß√£o Bayesiana (BIC) calculado, podemos identificar o melhor ajuste para o modelo de Mistura Bayesiana (BMM). O BIC √© uma m√©trica que avalia a qualidade de um modelo estat√≠stico, levando em considera√ß√£o sua complexidade e evitando o overfitting. Sua f√≥rmula √© expressa como:
# 
# $
# \begin{equation}
# BIC = -2 * \log(L) + k * \log(n)
# \end{equation}
# $
# 
# Onde $L$ representa a verossimilhan√ßa do modelo, ùëò √© o n√∫mero de par√¢metros e n √© o n√∫mero de observa√ß√µes. 
# 
# Um valor menor de BIC indica um modelo prefer√≠vel, sugerindo uma melhor adequa√ß√£o aos dados sem introduzir complexidade desnecess√°ria. Ao comparar diferentes modelos ou n√∫meros de componentes em uma mistura gaussiana, o BIC serve como uma ferramenta fundamental para determinar a escolha mais apropriada para os dados, possibilitando uma avalia√ß√£o quantitativa do ajuste desse tipo de modelo.

# In[38]:


results.sort_values('bic').head()


# Com $n_{components}$ e o tipo de covari√¢ncia definidos, podemos classificar nossos dados originais utilizando os clusters calculados.

# In[39]:


model = GaussianMixture(n_components=4, covariance_type='diag')
model.fit(teams)
labels = model.predict(teams)
vec_['labels'] = labels
df_teams_stats['labels'] = labels


# In[40]:


# A quantidade de times por clusters
grouped = df_teams_stats.groupby('labels')['index'].apply(list).reset_index()
grouped['count'] = grouped['index'].apply(len)

# Plotar o gr√°fico
plt.figure(figsize=(10, 6))
plt.bar(grouped['labels'], grouped['count'], color='skyblue')
plt.title('N√∫mero de Pa√≠ses por Cluster')
plt.xlabel('Labels')
plt.ylabel('N√∫mero de Pa√≠ses')
plt.xticks(grouped['labels'])
plt.grid(axis='y')

plt.show()


# In[41]:


boxplot_paises(df_teams_stats)
paises_por_cluster(df_teams_stats)


# ## Se√ß√£o 3.5: Agglomerative Clustering
# 
# O agrupamento hier√°rquico √© uma fam√≠lia de algoritmos que constr√≥i clusters aninhados por meio de fus√µes ou divis√µes sucessivas. Essa hierarquia de clusters √© representada como uma √°rvore, ou dendrograma, onde a raiz da √°rvore representa o √∫nico cluster que agrupa todas as amostras e as folhas correspondem aos clusters que cont√™m apenas uma amostra. 
# 
# O algoritmo de AgglomerativeClustering adota uma abordagem de baixo para cima: cada observa√ß√£o come√ßa em seu pr√≥prio cluster e os clusters s√£o sucessivamente mesclados. O crit√©rio de liga√ß√£o determina a m√©trica utilizada na estrat√©gia de mesclagem, podendo ser:
# 
# - **Ward**: minimiza a soma das diferen√ßas quadradas dentro de todos os clusters, sendo similar √† fun√ß√£o objetivo do k-means, mas aplicada de forma aglomerativa. Para dois clusters $C_i$ e $C_j$, a dist√¢ncia √© dada por:
# 
# $ 
# \begin{equation}
# d(C_i, C_j) = \sum_{x \in C_i} \sum_{y \in C_j} || x - y ||^2 
# \end{equation}
# $
# 
# - **Liga√ß√£o M√°xima (Complete Linkage)**: minimiza a dist√¢ncia m√°xima entre as observa√ß√µes de pares de clusters.
# 
# $
# \begin{equation}
# d(C_i, C_j) = \max_{x_a \in C_i, x_b \in C_j} || x_a - x_b || 
# \end{equation}
# $
# 
# - **Liga√ß√£o M√©dia (Average Linkage)**: minimiza a m√©dia das dist√¢ncias entre todas as observa√ß√µes de pares de clusters.
# 
# $ \begin{equation}
# d(C_i, C_j) = \min_{x_a \in C_i, x_b \in C_j} || x_a - x_b || 
# \end{equation}
# $
# 
# - **Liga√ß√£o √önica (Single Linkage)**: minimiza a dist√¢ncia entre as observa√ß√µes mais pr√≥ximas de pares de clusters.
# 
# $
# \begin{equation}
# d(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{x_a \in C_i} \sum_{x_b \in C_j} || x_a - x_b || 
# \end{equation}
# $

# In[42]:


plt.figure(figsize=(10, 7))
plt.title("Dendrograma Ward")
dendrogram = shc.dendrogram(shc.linkage(teams, method='ward', metric='euclidean'))
plt.axhline(y=6, color='r', linestyle='--')
plt.grid(False)
plt.show()


# In[43]:


plt.figure(figsize=(10, 7))
plt.title("Dendrograma Complete Linkage")
dendrogram = shc.dendrogram(shc.linkage(teams, method='complete', metric='euclidean'))
plt.axhline(y=3, color='r', linestyle='--')
plt.grid(False)
plt.show()


# In[44]:


plt.figure(figsize=(10, 7))
plt.title("Dendrograma Average Linkage")
dendrogram = shc.dendrogram(shc.linkage(teams, method='average', metric='euclidean'))
plt.axhline(y=2, color='r', linestyle='--')
plt.grid(False)
plt.show()


# In[45]:


plt.figure(figsize=(10, 7))
plt.title("Dendrograma Single Linkage")
dendrogram = shc.dendrogram(shc.linkage(teams, method='single', metric='euclidean'))
plt.axhline(y=1.20, color='r', linestyle='--')
plt.grid(False)
plt.show()


# Uma das principais vantagens do agrupamento hier√°rquico √© a visualiza√ß√£o dos resultados por meio de dendrogramas, que mostram a hierarquia das parti√ß√µes obtidas. No dendrograma, cada fus√£o de clusters √© representada por um link em forma de U, onde a altura do link indica a dist√¢ncia entre os clusters fundidos. Cortes horizontais no dendrograma permitem a identifica√ß√£o de diferentes parti√ß√µes, tornando a interpreta√ß√£o do agrupamento mais intuitiva.

# In[46]:


# Selecionando o corte de 3 clusters e classificando os dados originais
agc = AgglomerativeClustering(n_clusters=3)
labels = agc.fit_predict(teams)


# In[47]:


vec_['labels'] =labels
figures(vec_, title ='AgglomerativeClustering')


# In[48]:


# Classica os paises em grupos pelo AgglomerativeClustering
df_teams_stats['labels'] = labels
grouped = df_teams_stats.groupby('labels')['index'].apply(list).reset_index()
grouped['count'] = grouped['index'].apply(len)

# Plotar o gr√°fico
plt.figure(figsize=(10, 6))
plt.bar(grouped['labels'], grouped['count'], color='skyblue')
plt.title('N√∫mero de Pa√≠ses por Cluster')
plt.xlabel('Labels')
plt.ylabel('N√∫mero de Pa√≠ses')
plt.xticks(grouped['labels'])
plt.grid(axis='y')

plt.show()


# In[49]:


boxplot_paises(df_teams_stats)
paises_por_cluster(df_teams_stats)


# # Se√ß√£o 4: Conclus√£o
# 
# Neste trabalho, exploramos diversas t√©cnicas de redu√ß√£o de dimensionalidade, come√ßando com a An√°lise de Componentes Principais (PCA) para minimizar o custo operacional dos algoritmos em cen√°rios com grandes dimens√µes. Em seguida, aplicamos o algoritmo K-means, que permitiu classificar os dados em tr√™s clusters, priorizando a m√©trica silhouette_score, apesar de outras m√©tricas, como Davies-Bouldin e Calinski-Harabasz, sugerirem a forma√ß√£o de cinco clusters. Sinta-se livre para testar a forma√ß√£o de cinco clusters se desejar.
# 
# Ap√≥s a aplica√ß√£o do K-means, investigamos o n√∫mero m√≠nimo de vizinhos e a dist√¢ncia m√≠nima entre os clusters utilizando o DBScan. A abordagem de for√ßa bruta resultou em tr√™s clusters com par√¢metros de min_samples e eps de 18 e 1.06, respectivamente. O m√©todo baseado em NearestNeighbors tamb√©m concluiu a forma√ß√£o de tr√™s clusters, mas com min_samples de 9 e eps de 1.1722, evidenciando a robustez das an√°lises. Adicionalmente, aplicamos o algoritmo Agglomerative Cluster, que tamb√©m resultou em tr√™s clusters. Essa consist√™ncia entre os diferentes m√©todos de agrupamento ressalta a efic√°cia de nossa abordagem na identifica√ß√£o de padr√µes nos dados. O uso do agrupamento hier√°rquico permitiu observar a hierarquia dos clusters, enriquecendo a an√°lise com diversos metodos de mesclagem.
# 
# Por fim, a an√°lise dos gr√°ficos de boxplot revelou uma discrep√¢ncia significativa entre os clusters no que diz respeito ao n√∫mero de copas ganhas e suas classifica√ß√µes. Identificamos uma clara correla√ß√£o entre gols marcados e copas vencidas: o cluster 2 inclui times que sofreram muitos gols e conquistaram poucas copas, enquanto o cluster 0 abrange os times mais vitoriosos, funcionando como um intermedi√°rio entre os campe√µes e aqueles com desempenho inferior. Essa revis√£o sobre aprendizado n√£o supervisionado, englobando m√©todos de clusteriza√ß√£o como K-means, DBScan e Agglomerative Cluster, proporcionou insights valiosos sobre a estrutura dos dados e suas inter-rela√ß√µes, aprofundando nosso entendimento sobre o desempenho dos times na Copa do Mundo.

## Contato

#**Nome:** Lucas Oliveira Alves
#**Email:** [alves_lucasoliveira@usp.br](mailto:alves_lucasoliveira@usp.br)
#**LinkedIn:** [linkedin.com/in/cyblx](https://www.linkedin.com/in/cyblx/)
#**GitHub:** [github.com/cyblx](https://github.com/cyblx)
